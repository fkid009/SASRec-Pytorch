{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05aa047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import data_split, WrapBatch\n",
    "data_type = \"movielens\"\n",
    "user_num, item_num, Seq_train, Seq_val, Seq_test = data_split(data_type)\n",
    "\n",
    "sampler = WrapBatch(\n",
    "    Seq_train,\n",
    "    user_num,\n",
    "    item_num,\n",
    "    batch_size = 16,\n",
    "    max_len = 200,\n",
    "    n_workers = 2\n",
    ")\n",
    "\n",
    "u, seq, pos, neg = sampler.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d02633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Item Embedding (batch_size, max_len, K)\n",
    "# 2) Position Embedding (0 ~ 199) if max_len == 200 (batchsize, max_len, K)\n",
    "# 3) Zero-padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3fd8e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "629c60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointWiseFeedForward(nn.Module):\n",
    "    def __init__(self, \n",
    "                hidden_units,\n",
    "                dropout_rate):\n",
    "        super(PointWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(hidden_units, hidden_units, kernel_size = 1) # 입력(A, B, C) -> 출력(A, B, C)\n",
    "        self.dropout1 = nn.Dropout(p = dropout_rate)\n",
    "        self.relu = nn.relu()\n",
    "        self.conv2 = nn.Conv2d(hidden_units, hidden_units, kernel_size = 1)\n",
    "        self.dropout2 = nn.Dropout(p = dropout_rate)\n",
    "\n",
    "    def forward(self, inputs): # inputs: (batch_size, max_len, K)\n",
    "        inputs = inputs.transpose(-1, -2) # 마지막 차원 K와 마지막에서 두 번째 차원 max_len 변경 -> (B, K, max_len)\n",
    "        inputs = self.conv1(inputs)\n",
    "        inputs = self.dropout1(inputs)\n",
    "        inputs = self.relu(inputs)\n",
    "        inputs = self.conv2(inputs)\n",
    "        inputs = self.dropout2(inputs)\n",
    "        outputs = inputs.transpose(-1, -2) # 복원 (batch_size, max_len, K)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69044c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'module'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSASRec\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m      3\u001b[0m                 user_num, \n\u001b[0;32m      4\u001b[0m                 item_num, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m                 hidden_units,\n\u001b[0;32m     11\u001b[0m                 device):\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m K \u001b[38;5;241m%\u001b[39m num_heads \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'module'"
     ]
    }
   ],
   "source": [
    "class SASRec(nn.module):\n",
    "    def __init__(self,\n",
    "                user_num, \n",
    "                item_num, \n",
    "                K, \n",
    "                max_len, \n",
    "                dropout_rate,\n",
    "                num_blocks, \n",
    "                num_heads,\n",
    "                hidden_units,\n",
    "                first_norm, \n",
    "                device):\n",
    "\n",
    "        assert K % num_heads != 0\n",
    "\n",
    "        self.user_num = user_num\n",
    "        self.item_num = item_num\n",
    "        self.K = K\n",
    "        self.max_len = max_len\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_blocks = num_blocks # MHA 개수\n",
    "        self.num_heads = num_heads # Head width\n",
    "        self.hidden_units = hidden_units\n",
    "        self.first_norm = first_norm\n",
    "        self.device = device\n",
    "\n",
    "        self.item_emb = nn.Embedding(self.item_num + 1, self.K, padding_idx = 0)\n",
    "        self.pos_emb = nn.Embedding(self.max_len + 1, self.K, padding_idx = 0)\n",
    "        self.emb_dropout = nn.Dropout(p = self.dropout_rate)\n",
    "\n",
    "        self.attention_layernorms = nn.ModuleList()\n",
    "        self.attention_layers = nn.ModuleList()\n",
    "        self.forward_layernorms = nn.ModuleList()\n",
    "        self.forward_layers = nn.ModuleList()\n",
    "\n",
    "        self.last_layernorm = nn.LayerNorm(self.hidden_units, eps = 1e-8)\n",
    "\n",
    "        for _ in range(self.num_blocks):\n",
    "            new_attn_laternorm = nn.LayerNorm(self.hidden_units, eps = 1e-8)\n",
    "            self.attention_layernorms.append(new_attn_laternorm)\n",
    "\n",
    "            new_attn_layer = nn.MultiheadAttention(\n",
    "                self.hidden_units,\n",
    "                self.num_heads,\n",
    "                self.dropout_rate\n",
    "            )\n",
    "            self.attention_layers.append(new_attn_layer)\n",
    "\n",
    "            new_fwd_layer_norm = nn.LayerNorm(self.hidden_units, eps = 1e-8)\n",
    "            self.forward_layernorms.append(new_fwd_layer_norm)\n",
    "\n",
    "            new_fwd_layer = PointWiseFeedForward(self.hidden_units, self.dropout_rate)\n",
    "            self.forward_layers.append(new_fwd_layer)\n",
    "\n",
    "    def log2feats(self, logs):\n",
    "        seqs = self.item_emb(torch.LongTensor(logs).to(self.device))\n",
    "        seqs *= self.item_emb.embedding_dim ** 0.5 # normalization\n",
    "        \n",
    "        poss = np.tile(np.arange(1, logs.shape[1] + 1, [logs.shpe[0], 1]))\n",
    "        poss *= (logs != 0) # 로그 기록에서 padding 반영\n",
    "\n",
    "        seqs += self.pos_emb(torch.LongTensor(poss).to(self.device)) # 로그 임베딩 + 포지션 임베딩(learnable paramter)\n",
    "        seqs = self.emb_dropout(seqs)\n",
    "\n",
    "        tl = seqs.shape[1]\n",
    "        attention_mask = ~torch.tril(torch.ones(tl, tl), dtype = torch.bool, device = self.device)\n",
    "\n",
    "        for i in range(self.num_blocks):\n",
    "            seqs = torch.transpose(seqs, 0, 1) # (max_len, Batch_size, K)\n",
    "            if self.first_norm:\n",
    "                x = self.attention_layernorms[i](seqs)\n",
    "                mha_outputs, _ = self.attention_layers[i](\n",
    "                    x, x, x,\n",
    "                    attn_mask = attention_mask\n",
    "                )\n",
    "                seqs = seqs + mha_outputs # resudual\n",
    "                seqs = torch.traanspose(seqs, 0, 1) # (batch_size, max_len, K)\n",
    "                seqs = seqs + self.forward_layers[i](self.forward_layernorms[i](seqs))\n",
    "\n",
    "            else:\n",
    "                mha_outputs, _ = self.attention_layers[i](\n",
    "                    seqs, seqs, seqs,\n",
    "                    attn_mask = attention_mask\n",
    "                )\n",
    "                seqs = self.attention_layernorms[i](seqs + mha_outputs)\n",
    "                seqs = torch.transpose(seqs, 0, 1)\n",
    "                seqs = self.forward_layernorms[i](seqs + self.forward_layers[i](seqs))\n",
    "\n",
    "        outputs = self.last_layernorm(seqs)\n",
    "        return outputs\n",
    "    \n",
    "    def forward(self,\n",
    "                user_ids,\n",
    "                seqs,\n",
    "                pos_seqs,\n",
    "                neg_seqs):\n",
    "        log_feats = self.log2feats(seqs)\n",
    "\n",
    "        pos_embs = self.item_emb(torch.LongTensor(pos_seqs).to(self.device))\n",
    "        neg_embs = self.item_emb(torch.LongTensor(neg_seqs).to(self.device))\n",
    "\n",
    "        pos_logits = (log_feats * pos_embs).sum(dim = -1)\n",
    "        neg_logits = (log_feats ( neg_embs)).sum(dim = -1)\n",
    "\n",
    "        return pos_logits, neg_logits\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df770021",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 200\n",
    "\n",
    "item_emb = nn.Embedding(item_num + 1, 8, padding_idx = 0)\n",
    "pos_emb = nn.Embedding(max_len + 1, 8, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2289635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = np.array(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de906037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67a83563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,      0,      0,  ...,     28,    125,    538],\n",
       "        [     0,      0,      0,  ...,  79132, 112552, 139385],\n",
       "        [  2380,   5507,   1721,  ...,   2606,   7142,   6887],\n",
       "        ...,\n",
       "        [     0,      0,      0,  ...,   1270,   2407,   3033],\n",
       "        [     0,      0,      0,  ...,    515,    370,    594],\n",
       "        [ 60040,  61132,  49651,  ...,   3683,  71464,  81591]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98c1c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "position = np.tile(np.arange(1, seq.shape[1] + 1), [seq.shape[0], 1]) # 동일한 차원의 포지션 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "530956c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "position *= (seq != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e7889c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ..., 198, 199, 200],\n",
       "       [  0,   0,   0, ..., 198, 199, 200],\n",
       "       [  1,   2,   3, ..., 198, 199, 200],\n",
       "       ...,\n",
       "       [  0,   0,   0, ..., 198, 199, 200],\n",
       "       [  0,   0,   0, ..., 198, 199, 200],\n",
       "       [  1,   2,   3, ..., 198, 199, 200]], shape=(16, 200))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f01fdb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = seq.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfabceb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ff65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = ~torch.tril(torch.ones((tl, tl), dtype=torch.bool, device=self.dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da1c223f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 200, 8])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_emb(torch.LongTensor(seq)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faf176ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LHB/SASRec-practice/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LHB/SASRec-practice/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LHB/SASRec-practice/.venv/lib/python3.11/site-packages/torch/nn/modules/sparse.py:192\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LHB/SASRec-practice/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2542\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2536\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2537\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2538\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2539\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2540\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2541\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2542\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: index out of range in self"
     ]
    }
   ],
   "source": [
    "pos_emb(torch.LongTensor(pos)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c634352",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dcefef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "364376bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6, 5, 6, 5, 6])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(a, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aacf72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
