{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05aa047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import data_split, WrapBatch\n",
    "data_type = \"movielens\"\n",
    "user_num, item_num, Seq_train, Seq_val, Seq_test = data_split(data_type)\n",
    "\n",
    "sampler = WrapBatch(\n",
    "    Seq_train,\n",
    "    user_num,\n",
    "    item_num,\n",
    "    batch_size = 16,\n",
    "    max_len = 200,\n",
    "    n_workers = 2\n",
    ")\n",
    "\n",
    "u, seq, pos, neg = sampler.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d02633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Item Embedding (batch_size, max_len, K)\n",
    "# 2) Position Embedding (0 ~ 199) if max_len == 200 (batchsize, max_len, K)\n",
    "# 3) Zero-padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3fd8e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "629c60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointWiseFeedForward(nn.Module):\n",
    "    def __init__(self, \n",
    "                hidden_units,\n",
    "                dropout_rate):\n",
    "        super(PointWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(hidden_units, hidden_units, kernel_size = 1) # 입력(A, B, C) -> 출력(A, B, C)\n",
    "        self.dropout1 = nn.Dropout(p = dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(hidden_units, hidden_units, kernel_size = 1)\n",
    "        self.dropout2 = nn.Dropout(p = dropout_rate)\n",
    "\n",
    "    def forward(self, inputs): # inputs: (batch_size, max_len, K)\n",
    "        inputs = inputs.transpose(-1, -2) # 마지막 차원 K와 마지막에서 두 번째 차원 max_len 변경 -> (B, K, max_len)\n",
    "        inputs = self.conv1(inputs)\n",
    "        inputs = self.dropout1(inputs)\n",
    "        inputs = self.relu(inputs)\n",
    "        inputs = self.conv2(inputs)\n",
    "        inputs = self.dropout2(inputs)\n",
    "        outputs = inputs.transpose(-1, -2) # 복원 (batch_size, max_len, K)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c69044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SASRec(nn.Module):\n",
    "    def __init__(self,\n",
    "                user_num, \n",
    "                item_num, \n",
    "                K, \n",
    "                max_len, \n",
    "                dropout_rate,\n",
    "                num_blocks, \n",
    "                num_heads,\n",
    "                hidden_units,\n",
    "                first_norm, \n",
    "                device):\n",
    "        super().__init__()\n",
    "\n",
    "        assert K % num_heads == 0\n",
    "\n",
    "        self.user_num = user_num\n",
    "        self.item_num = item_num\n",
    "        self.K = K\n",
    "        self.max_len = max_len\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_blocks = num_blocks # MHA 개수\n",
    "        self.num_heads = num_heads # Head width\n",
    "        self.hidden_units = hidden_units\n",
    "        self.first_norm = first_norm\n",
    "        self.device = device\n",
    "\n",
    "        self.item_emb = nn.Embedding(self.item_num + 1, self.K, padding_idx = 0)\n",
    "        self.pos_emb = nn.Embedding(self.max_len + 1, self.K, padding_idx = 0)\n",
    "        self.emb_dropout = nn.Dropout(p = self.dropout_rate)\n",
    "\n",
    "        self.attention_layernorms = nn.ModuleList()\n",
    "        self.attention_layers = nn.ModuleList()\n",
    "        self.forward_layernorms = nn.ModuleList()\n",
    "        self.forward_layers = nn.ModuleList()\n",
    "\n",
    "        self.last_layernorm = nn.LayerNorm(self.hidden_units, eps = 1e-8)\n",
    "\n",
    "        for _ in range(self.num_blocks):\n",
    "            new_attn_laternorm = nn.LayerNorm(self.hidden_units, eps = 1e-8)\n",
    "            self.attention_layernorms.append(new_attn_laternorm)\n",
    "\n",
    "            new_attn_layer = nn.MultiheadAttention(\n",
    "                self.hidden_units,\n",
    "                self.num_heads,\n",
    "                self.dropout_rate\n",
    "            )\n",
    "            self.attention_layers.append(new_attn_layer)\n",
    "\n",
    "            new_fwd_layer_norm = nn.LayerNorm(self.hidden_units, eps = 1e-8)\n",
    "            self.forward_layernorms.append(new_fwd_layer_norm)\n",
    "\n",
    "            new_fwd_layer = PointWiseFeedForward(self.hidden_units, self.dropout_rate)\n",
    "            self.forward_layers.append(new_fwd_layer)\n",
    "\n",
    "    def log2feats(self, logs):\n",
    "        seqs = self.item_emb(torch.LongTensor(logs).to(self.device))\n",
    "        seqs *= self.item_emb.embedding_dim ** 0.5 # normalization\n",
    "        \n",
    "        poss = np.tile(np.arange(1, logs.shape[1] + 1, [logs.shpe[0], 1]))\n",
    "        poss *= (logs != 0) # 로그 기록에서 padding 반영\n",
    "\n",
    "        seqs += self.pos_emb(torch.LongTensor(poss).to(self.device)) # 로그 임베딩 + 포지션 임베딩(learnable paramter)\n",
    "        seqs = self.emb_dropout(seqs)\n",
    "\n",
    "        tl = seqs.shape[1]\n",
    "        attention_mask = ~torch.tril(torch.ones(tl, tl), dtype = torch.bool, device = self.device)\n",
    "\n",
    "        for i in range(self.num_blocks):\n",
    "            seqs = torch.transpose(seqs, 0, 1) # (max_len, Batch_size, K)\n",
    "            if self.first_norm:\n",
    "                x = self.attention_layernorms[i](seqs)\n",
    "                mha_outputs, _ = self.attention_layers[i](\n",
    "                    x, x, x,\n",
    "                    attn_mask = attention_mask\n",
    "                )\n",
    "                seqs = seqs + mha_outputs # resudual\n",
    "                seqs = torch.traanspose(seqs, 0, 1) # (batch_size, max_len, K)\n",
    "                seqs = seqs + self.forward_layers[i](self.forward_layernorms[i](seqs))\n",
    "\n",
    "            else:\n",
    "                mha_outputs, _ = self.attention_layers[i](\n",
    "                    seqs, seqs, seqs,\n",
    "                    attn_mask = attention_mask\n",
    "                )\n",
    "                seqs = self.attention_layernorms[i](seqs + mha_outputs)\n",
    "                seqs = torch.transpose(seqs, 0, 1)\n",
    "                seqs = self.forward_layernorms[i](seqs + self.forward_layers[i](seqs))\n",
    "\n",
    "        outputs = self.last_layernorm(seqs)\n",
    "        return outputs\n",
    "    \n",
    "    def forward(self,\n",
    "                user_ids,\n",
    "                seqs,\n",
    "                pos_seqs,\n",
    "                neg_seqs):\n",
    "        log_feats = self.log2feats(seqs)\n",
    "\n",
    "        pos_embs = self.item_emb(torch.LongTensor(pos_seqs).to(self.device))\n",
    "        neg_embs = self.item_emb(torch.LongTensor(neg_seqs).to(self.device))\n",
    "\n",
    "        pos_logits = (log_feats * pos_embs).sum(dim = -1)\n",
    "        neg_logits = (log_feats ( neg_embs)).sum(dim = -1)\n",
    "\n",
    "        return pos_logits, neg_logits\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ab44ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_num,\n",
    "        item_num,\n",
    "        K,\n",
    "        max_len,\n",
    "        dropout_rate,\n",
    "        num_blocks,\n",
    "        num_heads,\n",
    "        hidden_units,\n",
    "        first_norm,\n",
    "        device,\n",
    "    ):\n",
    "        super().__init__() \n",
    "\n",
    "        # K(임베딩 차원)과 head 수 체크\n",
    "        assert K % num_heads == 0\n",
    "        assert (\n",
    "            K == hidden_units\n",
    "        ), \"K(embedding dim)와 hidden_units는 동일하게 두는 게 안전합니다.\"\n",
    "\n",
    "        self.user_num = user_num\n",
    "        self.item_num = item_num\n",
    "        self.K = K\n",
    "        self.max_len = max_len\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_blocks = num_blocks  # MHA block 개수\n",
    "        self.num_heads = num_heads    # head 수\n",
    "        self.hidden_units = hidden_units  # = K\n",
    "        self.first_norm = first_norm      # Pre-LN 여부\n",
    "        self.device = device\n",
    "\n",
    "        # 임베딩: (item_id → K차원), (position → K차원)\n",
    "        self.item_emb = nn.Embedding(self.item_num + 1, self.hidden_units, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(self.max_len + 1, self.hidden_units, padding_idx=0)\n",
    "        self.emb_dropout = nn.Dropout(p=self.dropout_rate)\n",
    "\n",
    "        # Transformer block 구성 요소들\n",
    "        self.attention_layernorms = nn.ModuleList()\n",
    "        self.attention_layers = nn.ModuleList()\n",
    "        self.forward_layernorms = nn.ModuleList()\n",
    "        self.forward_layers = nn.ModuleList()\n",
    "\n",
    "        self.last_layernorm = nn.LayerNorm(self.hidden_units, eps=1e-8)\n",
    "\n",
    "        for _ in range(self.num_blocks):\n",
    "            # Self-Attention 앞/뒤에 들어갈 LN\n",
    "            new_attn_layernorm = nn.LayerNorm(self.hidden_units, eps=1e-8)\n",
    "            self.attention_layernorms.append(new_attn_layernorm)\n",
    "\n",
    "            # Multi-Head Self-Attention\n",
    "            new_attn_layer = nn.MultiheadAttention(\n",
    "                embed_dim=self.hidden_units,\n",
    "                num_heads=self.num_heads,\n",
    "                dropout=self.dropout_rate,\n",
    "                batch_first=False,  # (T, B, E) 형식 사용\n",
    "            )\n",
    "            self.attention_layers.append(new_attn_layer)\n",
    "\n",
    "            # FFN 앞/뒤에 들어갈 LN\n",
    "            new_fwd_layernorm = nn.LayerNorm(self.hidden_units, eps=1e-8)\n",
    "            self.forward_layernorms.append(new_fwd_layernorm)\n",
    "\n",
    "            # Position-wise FFN\n",
    "            new_fwd_layer = PointWiseFeedForward(self.hidden_units, self.dropout_rate)\n",
    "            self.forward_layers.append(new_fwd_layer)\n",
    "\n",
    "    def log2feats(self, logs):\n",
    "        \"\"\"\n",
    "        logs: (batch_size, max_len) 형태의 item id 시퀀스 (numpy array 또는 tensor)\n",
    "        return: (batch_size, max_len, hidden_units) 시퀀스 표현\n",
    "        \"\"\"\n",
    "        # numpy → LongTensor → device\n",
    "        if isinstance(logs, np.ndarray):\n",
    "            logs_t = torch.LongTensor(logs).to(self.device)\n",
    "        else:\n",
    "            logs_t = logs.long().to(self.device)\n",
    "\n",
    "        # (B, T, K)\n",
    "        seqs = self.item_emb(logs_t)\n",
    "        # 임베딩 스케일링 (Transformer 스타일)\n",
    "        seqs *= self.item_emb.embedding_dim ** 0.5\n",
    "\n",
    "        # position index: (B, T)\n",
    "        # [[1, 2, 3, ..., T],\n",
    "        #  [1, 2, 3, ..., T],\n",
    "        #   ... ]\n",
    "        poss = np.tile(\n",
    "            np.arange(1, logs_t.shape[1] + 1),\n",
    "            (logs_t.shape[0], 1)\n",
    "        )\n",
    "        # padding(0) 위치는 0으로 만들어서 pos_emb 영향 제거\n",
    "        poss *= (logs_t.cpu().numpy() != 0)\n",
    "\n",
    "        poss_t = torch.LongTensor(poss).to(self.device)\n",
    "        # item_emb + pos_emb\n",
    "        seqs = seqs + self.pos_emb(poss_t)  # (B, T, K)\n",
    "        seqs = self.emb_dropout(seqs)\n",
    "\n",
    "        # 인과 마스크 생성 (T, T)\n",
    "        tl = seqs.shape[1]\n",
    "        attention_mask = ~torch.tril(\n",
    "            torch.ones((tl, tl), dtype=torch.bool, device=self.device)\n",
    "        )\n",
    "        # MultiheadAttention은 (T, B, K)를 받으므로 transposed 상태에서 mask를 그대로 사용\n",
    "\n",
    "        # Transformer blocks\n",
    "        for i in range(self.num_blocks):\n",
    "            # (B, T, K) → (T, B, K)\n",
    "            seqs = torch.transpose(seqs, 0, 1)\n",
    "\n",
    "            if self.first_norm:\n",
    "                # Pre-LN 방식\n",
    "                x = self.attention_layernorms[i](seqs)  # (T, B, K)\n",
    "                mha_outputs, _ = self.attention_layers[i](\n",
    "                    x, x, x,\n",
    "                    attn_mask=attention_mask\n",
    "                )\n",
    "                seqs = seqs + mha_outputs  # residual\n",
    "                seqs = torch.transpose(seqs, 0, 1)  # (B, T, K)\n",
    "                seqs = seqs + self.forward_layers[i](self.forward_layernorms[i](seqs))\n",
    "            else:\n",
    "                # Post-LN 방식\n",
    "                mha_outputs, _ = self.attention_layers[i](\n",
    "                    seqs, seqs, seqs,\n",
    "                    attn_mask=attention_mask\n",
    "                )\n",
    "                seqs = self.attention_layernorms[i](seqs + mha_outputs)\n",
    "                seqs = torch.transpose(seqs, 0, 1)  # (B, T, K)\n",
    "                seqs = self.forward_layernorms[i](seqs + self.forward_layers[i](seqs))\n",
    "\n",
    "        # 마지막 LayerNorm (B, T, K)\n",
    "        outputs = self.last_layernorm(seqs)\n",
    "        return outputs\n",
    "\n",
    "    def forward(self,\n",
    "                seqs,      # (B, T)\n",
    "                pos_seqs,  # (B, T) positive target\n",
    "                neg_seqs   # (B, T) negative target\n",
    "                ):\n",
    "        # 시퀀스 → 시점별 hidden 표현\n",
    "        log_feats = self.log2feats(seqs)  # (B, T, K)\n",
    "\n",
    "        pos_seqs_t = torch.LongTensor(pos_seqs).to(self.device)\n",
    "        neg_seqs_t = torch.LongTensor(neg_seqs).to(self.device)\n",
    "\n",
    "        pos_embs = self.item_emb(pos_seqs_t)  # (B, T, K)\n",
    "        neg_embs = self.item_emb(neg_seqs_t)  # (B, T, K)\n",
    "\n",
    "        # 시점별 dot product → 각 타임스텝의 (positive/negative) logit\n",
    "        pos_logits = (log_feats * pos_embs).sum(dim=-1)  # (B, T)\n",
    "        neg_logits = (log_feats * neg_embs).sum(dim=-1)  # (B, T)\n",
    "\n",
    "        return pos_logits, neg_logits#, log_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df770021",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 200\n",
    "\n",
    "item_emb = nn.Embedding(item_num + 1, 8, padding_idx = 0)\n",
    "pos_emb = nn.Embedding(max_len + 1, 8, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2289635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = np.array(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de906037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67a83563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,      0,      0,  ...,     28,    125,    538],\n",
       "        [     0,      0,      0,  ...,  79132, 112552, 139385],\n",
       "        [  2380,   5507,   1721,  ...,   2606,   7142,   6887],\n",
       "        ...,\n",
       "        [     0,      0,      0,  ...,   1270,   2407,   3033],\n",
       "        [     0,      0,      0,  ...,    515,    370,    594],\n",
       "        [ 60040,  61132,  49651,  ...,   3683,  71464,  81591]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98c1c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "position = np.tile(np.arange(1, seq.shape[1] + 1), [seq.shape[0], 1]) # 동일한 차원의 포지션 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "530956c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "position *= (seq != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e7889c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ..., 198, 199, 200],\n",
       "       [  0,   0,   0, ..., 198, 199, 200],\n",
       "       [  1,   2,   3, ..., 198, 199, 200],\n",
       "       ...,\n",
       "       [  0,   0,   0, ..., 198, 199, 200],\n",
       "       [  0,   0,   0, ..., 198, 199, 200],\n",
       "       [  1,   2,   3, ..., 198, 199, 200]], shape=(16, 200))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f01fdb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = seq.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfabceb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ff65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = ~torch.tril(torch.ones((tl, tl), dtype=torch.bool, device=self.dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da1c223f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 200, 8])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_emb(torch.LongTensor(seq)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faf176ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LHB/SASRec-practice/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LHB/SASRec-practice/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LHB/SASRec-practice/.venv/lib/python3.11/site-packages/torch/nn/modules/sparse.py:192\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LHB/SASRec-practice/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2542\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2536\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2537\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2538\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2539\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2540\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2541\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2542\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: index out of range in self"
     ]
    }
   ],
   "source": [
    "pos_emb(torch.LongTensor(pos)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c634352",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dcefef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "364376bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6, 5, 6, 5, 6])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(a, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21aacf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "batcher = WrapBatch(Seq_train, user_num, item_num, batch_size = 16)\n",
    "model = SASRec(user_num = user_num,\n",
    "                item_num = item_num,\n",
    "                K = 16,\n",
    "                max_len = 200,\n",
    "                dropout_rate = 0.1,\n",
    "                num_blocks = 2,\n",
    "                num_heads = 1,\n",
    "                hidden_units = 16,\n",
    "                first_norm = True,\n",
    "                device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "566a7c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-4)\n",
    "num_epochs = 2\n",
    "num_batch = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "addc6d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "u, seq, pos, neg = np.array(u), np.array(seq), np.array(pos), np.array(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f8d3c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = model(seq, pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08577801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.0000,   0.0000,   0.0000,  ...,  -2.2961,   0.6431,   1.8700],\n",
       "        [ -4.4368,   7.4203,  -0.0522,  ...,  -5.6885,  -0.8233,   8.0495],\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,  -2.5846,  -4.8858,  -6.8889],\n",
       "        ...,\n",
       "        [  1.6703,  -0.1006,  -0.9466,  ...,  -4.2049,   2.3656,   0.5574],\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   4.1725,   5.3107, -13.6621],\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.6092,   5.3331,   2.5382]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2612104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 200])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8942bda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 200, 16])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6db7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4548,  0.0203,  1.7574,  0.2292, -0.4827,  0.2113, -0.5580,  1.7425,\n",
       "         0.2174,  0.8339, -2.4244,  0.4552, -0.2461, -0.5008,  0.4849, -1.2853],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d83a117",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels, neg_labels = torch.ones(a.shape, device = \"cpu\"), torch.ones(b.shape, device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7bf15604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(pos_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "593530fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def evaluate(model, dataset,\n",
    "            max_len: int = 200,\n",
    "            is_test: bool = False):\n",
    "\n",
    "    train, val, test, user_num, item_num = dataset.copy()\n",
    "\n",
    "    NDCG, HT = 0.0, 0.0\n",
    "    valid_user = 0.0\n",
    "    \n",
    "    if user_num > 10000:\n",
    "        users = random.sample(range(1, user_num + 1), 10000)\n",
    "    else:\n",
    "        users = range(1, user_num + 1)\n",
    "\n",
    "    for u in users:\n",
    "        if len(train[u]) < 1 or len(test[u]) < 1: continue\n",
    "\n",
    "        seq = np.zeros([max_len], dtype = np.int32) # history 구성\n",
    "        idx = max_len - 1\n",
    "        if is_test:\n",
    "            seq[idx] = val[u][0]\n",
    "            idx -= 1\n",
    "        for i in reversed(train[u]):\n",
    "            seq[idx] = i\n",
    "            idx -= 1\n",
    "            if idx == -1: break\n",
    "\n",
    "        rated = set(train[u]) \n",
    "        rated.add(0)\n",
    "\n",
    "        item_idx = [test[u][0]] if is_test else [val[u][0]]# 정답\n",
    "        for _ in range(100):\n",
    "            t = np.random.randint(1, item_num + 1)\n",
    "            while t in rated: t = np.random.randint(1, item_num + 1)\n",
    "            item_idx.append(t)\n",
    "\n",
    "        predictions = - model.predict(*[np.array(l) for l in [[u], [seq], item_idx]]) # negative score for ascending\n",
    "        predictions = predictions[0] # 정답 아이템에 대한 스코어\n",
    "\n",
    "        rank = predictions.argsort().argsort()[0].item() # 정답 아이템이 해당 스코어들 상에서의 랭킹, int\n",
    "\n",
    "        valid_user += 1\n",
    "\n",
    "        if rank < 10:\n",
    "            NDCG += 1 / np.log2(rank + 2)\n",
    "            HT += 1\n",
    "\n",
    "    return NDCG / valid_user, HT / valid_user\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_ndcg, best_val_hr = 0.0, 0.0\n",
    "best_test_ndcg, best_test_hr = 0.0, 0.0\n",
    "# T = 0.0 # 학습 경과 누적 시간 (total elapsed time)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for step in range(num_batch):\n",
    "        u, seq, pos, neg = sampler.next_batch()\n",
    "        u, seq, pos, neg = np.array(u), np.array(seq), np.array(pos), np.array(neg)\n",
    "        pos_logits, neg_logits = model(seq, pos, neg)\n",
    "        pos_labels, neg_lables = torch.ones(pos_logits.shape, device = \"cpu\"), torch.zeros(neg_logits.shape, device = \"cpu\")\n",
    "        \n",
    "        indices = np.where(pos != 0)\n",
    "        optimizer.zero_grad()\n",
    "        loss = bce_criterion(pos_logits[indices], pos_labels[indices])\n",
    "        loss += bce_criterion(neg_logits[indices], neg_labels[indices])\n",
    "\n",
    "        # for param in model.item_emb.parameters():\n",
    "        #     loss += args.l2_emb * torch.sum(param ** 2) # L2정규화\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if step % 2 == 0: # 2 epoch 마다 평가\n",
    "        model.eval()\n",
    "        print(\"Evaluating...\")\n",
    "        val_NDCG, val_HR = evaluate(model, dataset, max_len, is_test = False)\n",
    "        test_NDCG, test_HR = evaluate(model, dataset, max_len, is_test = True)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ade19ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "# 1) 유저 시퀀스 구성, validation 경우, history: train, test의 경우, history: train + validation\n",
    "# 2) history를 고정 길이의 seq로 변환\n",
    "# 3) candidate 아이템 1개 만들기: 정답 1개(validation, test) + negative 100개(train아이테 제외, 랜덤하게)\n",
    "# 4) 모델이 각 candidate 아이템의 점수를 계산\n",
    "# 5) 각 아이템의 ranking 계산 및 정렬\n",
    "# 6) Top-K 안에 들어오는지 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e41fb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
