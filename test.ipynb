{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05aa047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import data_split, WrapBatch\n",
    "data_type = \"movielens\"\n",
    "user_num, item_num, Seq_train, Seq_val, Seq_test = data_split(data_type)\n",
    "\n",
    "sampler = WrapBatch(\n",
    "    Seq_train,\n",
    "    user_num,\n",
    "    item_num,\n",
    "    batch_size = 16,\n",
    "    max_len = 200,\n",
    "    n_workers = 2\n",
    ")\n",
    "\n",
    "u, seq, pos, neg = sampler.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d02633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Item Embedding (batch_size, max_len, K)\n",
    "# 2) Position Embedding (0 ~ 199) if max_len == 200 (batchsize, max_len, K)\n",
    "# 3) Zero-padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3fd8e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointWiseFeedForward(nn.Module):\n",
    "    def __init__(self, \n",
    "                hidden_units,\n",
    "                dropout_rate):\n",
    "        super(PointWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(hidden_units, hidden_units, kernel_size = 1) # 입력(A, B, C) -> 출력(A, B, C)\n",
    "        self.dropout1 = nn.Dropout(p = dropout_rate)\n",
    "        self.relu = nn.relu()\n",
    "        self.conv2 = nn.Conv2d(hidden_units, hidden_units, kernel_size = 1)\n",
    "        self.dropout2 = nn.Dropout(p = dropout_rate)\n",
    "\n",
    "    def forward(self, inputs): # inputs: (batch_size, max_len, K)\n",
    "        inputs = inputs.transpose(-1, -2) # 마지막 차원 K와 마지막에서 두 번째 차원 max_len 변경 -> (B, K, max_len)\n",
    "        inputs = self.conv1(inputs)\n",
    "        inputs = self.dropout1(inputs)\n",
    "        inputs = self.relu(inputs)\n",
    "        inputs = self.conv2(inputs)\n",
    "        inputs = self.dropout2(inputs)\n",
    "        outputs = inputs.transpose(-1, -2) # 복원 (batch_size, max_len, K)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SASRec(nn.module):\n",
    "    def __init__(self,\n",
    "                user_num, \n",
    "                item_num, \n",
    "                K, \n",
    "                max_len, \n",
    "                dropout_rate,\n",
    "                num_blocks, \n",
    "                num_heads,\n",
    "                hidden_units,\n",
    "                device):\n",
    "\n",
    "        assert K % num_heads != 0\n",
    "\n",
    "        self.user_num = user_num\n",
    "        self.item_num = item_num\n",
    "        self.K = K\n",
    "        self.max_len = max_len\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_blocks = num_blocks # MHA 개수\n",
    "        self.num_heads = num_heads # Head width\n",
    "        self.hidden_units = hidden_units\n",
    "        self.device = device\n",
    "\n",
    "        self.item_emb = nn.Embedding(self.item_num + 1, self.K, padding_idx = 0)\n",
    "        self.pos_emb = nn.Embedding(self.max_len + 1, self.K, padding_idx = 0)\n",
    "        self.emb_dropout = nn.Dropout(p = self.dropout_rate)\n",
    "\n",
    "        self.attention_layernorms = nn.ModuleList()\n",
    "        self.attention_layers = nn.ModuleList()\n",
    "        self.forward_layernorms = nn.ModuleList()\n",
    "        self.forward_layers = nn.ModuleList()\n",
    "\n",
    "        for _ in range(self.num_blocks):\n",
    "            new_attn_laternorm = nn.LayerNorm(self.hidden_units, eps = 1e-8)\n",
    "            self.attention_layernorms.append(new_attn_laternorm)\n",
    "\n",
    "            new_attn_layer = nn.MultiheadAttention(\n",
    "                self.hidden_units,\n",
    "                self.num_heads,\n",
    "                self.dropout_rate\n",
    "            )\n",
    "            self.attention_layers.append(new_attn_layer)\n",
    "\n",
    "            new_fwd_layer_norm = nn.LayerNorm(self.hidden_units, eps = 1e-8)\n",
    "            self.forward_layernorms.append(new_fwd_layer_norm)\n",
    "\n",
    "            new_fwd_layer = PointWiseFeedForward(self.hidden_units, self.dropout_rate)\n",
    "            self.forward_layers.append(new_fwd_layer)\n",
    "\n",
    "    def log2feats(self, logs):\n",
    "        seqs = self.item_emb(torch.LongTensor(logs).to(self.device))\n",
    "        seqs *= self.item_emb.embedding_dim ** 0.5 # normalization\n",
    "        \n",
    "        poss = np.tile(np.arange(1, logs.shape[1] + 1, [logs.shpe[0], 1]))\n",
    "        poss *= (logs != 0) # 로그 기록에서 padding 반영\n",
    "\n",
    "        seqs += self.pos_emb(torch.LongTensor(poss).to(self.device)) # 로그 임베딩 + 포지션 임베딩(learnable paramter)\n",
    "        seqs = self.emb_dropout(seqs)\n",
    "\n",
    "        seq_len = seqs.shape[1]\n",
    "\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df770021",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 200\n",
    "\n",
    "item_emb = nn.Embedding(item_num + 1, 8, padding_idx = 0)\n",
    "pos_emb = nn.Embedding(max_len + 1, 8, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2289635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = np.array(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de906037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67a83563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,      0,      0,  ...,    798,     66,    889],\n",
       "        [     0,      0,      0,  ...,    342,      7,    552],\n",
       "        [     0,      0,      0,  ...,   1136,  98809, 170939],\n",
       "        ...,\n",
       "        [     0,      0,      0,  ...,   3994,   5989,  54004],\n",
       "        [     0,      0,      0,  ...,      2,    150,    333],\n",
       "        [ 94777,  95307,  97921,  ...,  65130,   1721,   4022]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98c1c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "position = np.tile(np.arange(1, seq.shape[1] + 1), [seq.shape[0], 1]) # 동일한 차원의 포지션 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "530956c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "position *= (seq != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e7889c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ..., 198, 199, 200],\n",
       "       [  0,   0,   0, ..., 198, 199, 200],\n",
       "       [  0,   0,   0, ..., 198, 199, 200],\n",
       "       ...,\n",
       "       [  0,   0,   0, ..., 198, 199, 200],\n",
       "       [  0,   0,   0, ..., 198, 199, 200],\n",
       "       [  1,   2,   3, ..., 198, 199, 200]], shape=(16, 200))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01fdb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da1c223f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 200, 8])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_emb(torch.LongTensor(seq)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faf176ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LHB/SASRec-practice/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LHB/SASRec-practice/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LHB/SASRec-practice/.venv/lib/python3.11/site-packages/torch/nn/modules/sparse.py:192\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LHB/SASRec-practice/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2542\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2536\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2537\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2538\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2539\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2540\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2541\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2542\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: index out of range in self"
     ]
    }
   ],
   "source": [
    "pos_emb(torch.LongTensor(pos)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c634352",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dcefef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "364376bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6, 5, 6, 5, 6])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(a, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aacf72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
